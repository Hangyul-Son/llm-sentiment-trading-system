{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec66573-9c1b-4f17-bcbe-787ddf067576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "openai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key not found. Please check the .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba6723-bf59-4f20-9dc8-856319891aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Instantiate the client using environment variable for the API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Make a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1e615-b023-4724-9838-708afc0e4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f842945-caf6-49ee-a682-b90aa99244b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mood_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the mood of this message, focusing on any hypothetical or speculative language that could affect sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def institutional_investor_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze this message as if you are an institutional investor, focusing on long-term impacts on stability and growth potential: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def individual_investor_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze this message as if you are an individual investor, focusing on short-term price impact and immediate gains or losses: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rhetoric_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the rhetorical style of this message, such as sarcasm, exaggeration, or assertive statements, and how these elements affect sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def dependency_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Focus on the speakerâ€™s sentiment in this message, without considering external perspectives or opinions of third parties: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def aspect_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the sentiment toward the main entity (e.g., company or stock ticker) in this message, ignoring unrelated information: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def reference_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Identify references to time, price points, or external factors in this message, and analyze how they impact the overall sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e817c-9884-446c-9e84-8725b5b39665",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"$SBUX this stock has been consolidating and coiling for years. I think many underestimate it. spring how\"\n",
    "\n",
    "# Run each agent on the content\n",
    "responses = {\n",
    "    \"mood_agent\": mood_agent(content),\n",
    "    \"institutional_investor_agent\": institutional_investor_agent(content),\n",
    "    \"individual_investor_agent\": individual_investor_agent(content),\n",
    "    \"rhetoric_agent\": rhetoric_agent(content),\n",
    "    \"dependency_agent\": dependency_agent(content),\n",
    "    \"aspect_agent\": aspect_agent(content),\n",
    "    \"reference_agent\": reference_agent(content)\n",
    "}\n",
    "\n",
    "# Display the responses from each agent\n",
    "for agent, response in responses.items():\n",
    "    print(f\"{agent}: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f3dd9-0bfd-442d-ac8a-1fd74210842a",
   "metadata": {},
   "source": [
    "# Mock Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf73599-d05e-4915-b837-eee6bf44013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial message for sentiment analysis\n",
    "content = \"$SBUX this stock has been consolidating and coiling for years. I think many underestimate it. spring how\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6d20dc-de06-4699-897a-68c05b1cc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock agent responses to simulate natural language sentiment descriptions\n",
    "def mood_agent(content):\n",
    "    return \"The mood seems cautiously optimistic about future potential.\"\n",
    "\n",
    "def institutional_investor_agent(content):\n",
    "    return \"This message suggests a long-term positive outlook due to consolidation.\"\n",
    "\n",
    "def individual_investor_agent(content):\n",
    "    return \"Indicates potential for short-term gains but remains uncertain.\"\n",
    "\n",
    "def rhetoric_agent(content):\n",
    "    return \"The language implies underestimation by others, hinting at overlooked growth.\"\n",
    "\n",
    "def dependency_agent(content):\n",
    "    return \"The speaker seems to hold a positive perspective independently.\"\n",
    "\n",
    "def aspect_agent(content):\n",
    "    return \"Focuses specifically on $SBUX, suggesting stability and possible growth.\"\n",
    "\n",
    "def reference_agent(content):\n",
    "    return \"No explicit timeframes, but consolidation indicates a buildup for potential movement.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4b2e87-a11a-42f9-b8d8-82b2519b3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect initial responses from each agent in a dictionary\n",
    "responses = {\n",
    "    \"mood_agent\": mood_agent(content),\n",
    "    \"institutional_investor_agent\": institutional_investor_agent(content),\n",
    "    \"individual_investor_agent\": individual_investor_agent(content),\n",
    "    \"rhetoric_agent\": rhetoric_agent(content),\n",
    "    \"dependency_agent\": dependency_agent(content),\n",
    "    \"aspect_agent\": aspect_agent(content),\n",
    "    \"reference_agent\": reference_agent(content)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "030a5628-175e-4e89-b5f4-1b82c34a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summative_agent(responses, max_rounds=2):\n",
    "    round_count = 0\n",
    "    consensus_reached = False\n",
    "    high_priority_agents = [\"institutional_investor_agent\", \"individual_investor_agent\"]\n",
    "    \n",
    "    # Store initial responses\n",
    "    sentiment_summary = {agent: response for agent, response in responses.items()}\n",
    "\n",
    "    while not consensus_reached and round_count < max_rounds:\n",
    "        # Step 1: Ask the model to summarize the collective sentiment based on agent responses\n",
    "        combined_responses = \"\\n\".join([f\"{agent}: {response}\" for agent, response in sentiment_summary.items()])\n",
    "        \n",
    "        # Ask GPT-4 to interpret the overall sentiment, allowing for Positive, Negative, Neutral, or Mixed\n",
    "        overall_sentiment = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Based on the following responses from various agents, summarize the overall sentiment as Positive, Negative, Neutral, or Mixed if no clear conclusion can be derived:\\n\\n{combined_responses}\"\n",
    "            }]\n",
    "        ).choices[0].message.content.strip().lower()\n",
    "        \n",
    "        # Check if a clear consensus is indicated by the response\n",
    "        if \"positive\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Positive\"\n",
    "        elif \"negative\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Negative\"\n",
    "        elif \"neutral\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Neutral\"\n",
    "        elif \"mixed\" in overall_sentiment or not consensus_reached:\n",
    "            # Inconclusive: proceed to the next round\n",
    "            round_count += 1\n",
    "            for agent, response in responses.items():\n",
    "                refined_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": f\"Refine your sentiment analysis by reviewing these responses:\\n{combined_responses}\\nOriginal response: {response}\"}\n",
    "                    ]\n",
    "                )\n",
    "                sentiment_summary[agent] = refined_response.choices[0].message.content\n",
    "\n",
    "    # If no consensus after max rounds, defer to high-priority agents for final decision\n",
    "    if not consensus_reached:\n",
    "        high_priority_responses = \"\\n\".join([f\"{agent}: {sentiment_summary[agent]}\" for agent in high_priority_agents if agent in sentiment_summary])\n",
    "        final_sentiment = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Summarize the final sentiment based on high-priority agents alone, as Positive, Negative, Neutral, or Mixed if no clear conclusion can be drawn:\\n{high_priority_responses}\"\n",
    "            }]\n",
    "        ).choices[0].message.content.strip().capitalize()\n",
    "\n",
    "    # Return the final sentiment and a detailed summary of agent responses\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab36fcf-fb24-464e-ad82-24a830e00dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the summative agent to determine final sentiment\n",
    "final_output = summative_agent(responses)\n",
    "print(\"Final Aggregated Sentiment:\", final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371320e9",
   "metadata": {},
   "source": [
    "# Financial Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6724a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKED_KEYWORDS = [\n",
    "    [\"HSI\", \"Hang Seng\", \"Hong Kong Stocks\", \"Hong Kong Index\", \"Hang Seng Index\"],  # Rank 1\n",
    "    [\"China\", \"Asia\", \"Singapore\"],      # Rank 2\n",
    "    [\"US\", \"Global\", \"Something\"]        # Rank 3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25488d18",
   "metadata": {},
   "source": [
    "### Financial News: GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "params = {\n",
    "    'query': '(economy OR finance)',  # Corrected query with parentheses\n",
    "    'mode': 'ArtList',\n",
    "    'format': 'JSON'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# Sending request to GDELT\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "# Verify the response and process the data\n",
    "if response.status_code == 200:\n",
    "    if response.text.strip():  # Check if the response is not empty\n",
    "        try:\n",
    "            data = response.json()\n",
    "            articles = pd.DataFrame(data.get('articles', []))\n",
    "            if 'title' in articles.columns:\n",
    "                # Displaying only the titles\n",
    "                display(articles['title'].head())\n",
    "            else:\n",
    "                print(\"No title information found in the response.\")\n",
    "        except ValueError:\n",
    "            print(\"Error decoding JSON. Response text:\", response.text)\n",
    "    else:\n",
    "        print(\"Received an empty response.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code, \"\\nResponse Text: \", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_data_with_ranking(max_records=10):\n",
    "    credible_sources = ['reuters.com', 'yahoo.com', 'cnbc.com']\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        query = f\"({ ' OR '.join(keywords) })\"\n",
    "        print(f\"Generated GDELT query: {query}\")\n",
    "        articles = fetch_gdelt_data(query, max_records)\n",
    "        print(articles)\n",
    "        if articles:\n",
    "            filtered_articles = [\n",
    "                {\n",
    "                    \"title\": article.get(\"title\"),\n",
    "                    \"url\": article.get(\"url\"),\n",
    "                    \"language\": article.get(\"language\"),\n",
    "                    \"sourcecountry\": article.get(\"sourcecountry\"),\n",
    "                    \"domain\": article.get(\"domain\")\n",
    "                }\n",
    "                for article in articles\n",
    "                if any(source in article.get('url', '') for source in credible_sources) and article.get('language', '').lower() == 'english'\n",
    "            ]\n",
    "            if filtered_articles:\n",
    "                print(f\"Found articles with Rank {rank} keywords from credible sources.\")\n",
    "                return filtered_articles\n",
    "\n",
    "    print(\"No articles found for any of the ranked keywords from credible sources.\")\n",
    "    return []\n",
    "\n",
    "def fetch_gdelt_data(query, max_records=10):\n",
    "    url = f\"https://api.gdeltproject.org/api/v2/doc/doc?query={query}&mode=artlist&maxrecords={max_records}&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        if response.headers.get('Content-Type') == 'application/json':\n",
    "            try:\n",
    "                data = response.json()\n",
    "                return data.get('articles', [])\n",
    "            except JSONDecodeError as e:\n",
    "                print(\"Error decoding JSON response from GDELT:\", e)\n",
    "                return []\n",
    "        else:\n",
    "            print(\"Unexpected content type from GDELT response:\", response.headers.get('Content-Type'))\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching GDELT data:\", e)\n",
    "        return []\n",
    "    \n",
    "\n",
    "fetch_gdelt_data_with_ranking(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90096d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the GDELT API endpoint\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Define the query parameters for the GDELT GEO 2.0 API\n",
    "params = {\n",
    "    \"query\": '(Hong Kong OR HSI OR \"Hang Seng\") AND (China OR Asia OR Singapore) AND (US OR Global OR Something) ' \\\n",
    "             'AND (domain:reuters.com OR domain:yahoo.com OR domain:cnbc.com) AND sourcelang:english',\n",
    "    \"mode\": \"ArtList\",          # To get a list of articles\n",
    "    \"format\": \"json\",           # To retrieve data in JSON format\n",
    "    \"maxrecords\": \"10\",         # Limit to 10 records for testing\n",
    "    \"timespan\": \"7d\",           # Only articles from the past week\n",
    "    \"sort\": \"DateDesc\"          # Sort by most recent articles first\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Print the response to inspect or troubleshoot\n",
    "print(response.text)\n",
    "\n",
    "# Process the JSON response if successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data.get('articles', [])\n",
    "    for article in articles:\n",
    "        print(f\"Title: {article.get('title')}, URL: {article.get('url')}\")\n",
    "else:\n",
    "    print(\"Error:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_data_with_ranking(max_records=10):\n",
    "    \"\"\"Fetches articles from GDELT based on ranked keywords.\"\"\"\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        # Adjusted without quotes around OR terms\n",
    "        keyword_query = \"(\" + \" OR \".join(keywords) + \")\"\n",
    "        query = f'{keyword_query} AND domain:reuters.com AND sourcelang:english'\n",
    "        print(f\"Trying Rank {rank} keywords: {query}\")\n",
    "        articles = fetch_gdelt_data(query, max_records)\n",
    "        if articles:\n",
    "            print(f\"Found articles with Rank {rank} keywords.\")\n",
    "            return pd.DataFrame(articles)\n",
    "    print(\"No articles found.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def fetch_gdelt_data(query, max_records=10):\n",
    "    \"\"\"Helper function to retrieve GDELT data for a specific query.\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"format\": \"json\",\n",
    "        \"maxrecords\": max_records,\n",
    "        \"sort\": \"DateDesc\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        \n",
    "        # Print available fields in each article for inspection\n",
    "        for article in articles:\n",
    "            print(article.keys())  # Print keys for each article\n",
    "            article[\"content\"] = article.get(\"excerpt\", \"Content not available\")  # Use 'excerpt' if 'content' is missing\n",
    "            \n",
    "        return articles\n",
    "    else:\n",
    "        print(\"Error fetching GDELT data:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    \n",
    "print(\"\\nTesting GDELT data retrieval...\")\n",
    "gdelt_data = fetch_gdelt_data_with_ranking()\n",
    "display(gdelt_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652a7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Rank 1 keywords: (HSI OR Hang Seng OR Hong Kong Stocks OR Hong Kong Index OR Hang Seng Index) AND (domain:reuters.com OR domain:yahoo.com OR domain:cnbc.com) AND sourcelang:english\n",
      "{\"articles\": [ { \"url\": \"https://finance.yahoo.com/news/schwab-crypto-alleged-fraud-elderly-205351006.html\", \"url_mobile\": \"\", \"title\": \"Schwab , crypto , alleged fraud and an elderly couple $18 . 5M loss\", \"seendate\": \"20241025T070000Z\", \"socialimage\": \"https://s.yimg.com/ny/api/res/1.2/LJ28WXHYy0R.DTGI4HuhAw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/financial_planning_664/bfe3e68a1daa6197460690e115bb30c8\", \"domain\": \"finance.yahoo.com\", \"language\": \"English\", \"sourcecountry\": \"United States\" }] }\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_data_per_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching GDELT data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfetch_gdelt_data_with_ranking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mfetch_gdelt_data_with_ranking\u001b[1;34m(max_records)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying Rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m articles \u001b[38;5;241m=\u001b[39m fetch_gdelt_data(query, max_records)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnum_data_per_source\u001b[49m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Print the first article's keys for inspection\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample article keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m, articles[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(articles)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_data_per_source' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import requests\n",
    "def fetch_gdelt_data_with_ranking(max_records=10):\n",
    "    \"\"\"Fetches articles from GDELT based on ranked keywords in the past hour.\"\"\"\n",
    "    one_hour_ago = datetime.now(pytz.utc) - timedelta(hours=1)\n",
    "    one_hour_ago_str = one_hour_ago.strftime(\"%Y%m%d%H%M%S\")  # Format suitable for GDELT (e.g., 20241010153000)\n",
    "\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        keyword_query = \"(\" + \" OR \".join(keywords) + \")\"\n",
    "        # Adding date range filter to GDELT query\n",
    "        query = f'{keyword_query} AND (domain:reuters.com OR domain:yahoo.com OR domain:cnbc.com) AND sourcelang:english'\n",
    "        print(f\"Trying Rank {rank} keywords: {query}\")\n",
    "        articles = fetch_gdelt_data(query, max_records)\n",
    "        \n",
    "        if len(articles) >= num_data_per_source:\n",
    "            # Print the first article's keys for inspection\n",
    "            print(\"Sample article keys:\", articles[0].keys())\n",
    "            return pd.DataFrame(articles)\n",
    "\n",
    "    print(\"No articles found in the past hour.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def fetch_gdelt_data(query, max_records=10):\n",
    "    \"\"\"Helper function to retrieve GDELT data for a specific query.\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"format\": \"json\",\n",
    "        \"maxrecords\": max_records,\n",
    "        \"sort\": \"DateDesc\",\n",
    "        \"timespan\": \"1h\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        print(response.text)\n",
    "        data = response.json()\n",
    "        return data.get('articles', [])\n",
    "    else:\n",
    "        print(\"Error fetching GDELT data:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "print(fetch_gdelt_data_with_ranking())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f65662",
   "metadata": {},
   "source": [
    "### SNS Financial Data: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da381b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Set your keys and tokens (replace placeholders with your actual credentials)\n",
    "consumer_key = \"YOUR_CONSUMER_KEY\"\n",
    "consumer_secret = \"YOUR_CONSUMER_SECRET\"\n",
    "access_token = \"YOUR_ACCESS_TOKEN\"\n",
    "access_token_secret = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Fetch tweets with the specified keyword\n",
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search_tweets, q=\"economy\", lang=\"en\", tweet_mode=\"extended\").items(10):\n",
    "    tweets.append({'created_at': tweet.created_at, 'user': tweet.user.screen_name, 'text': tweet.full_text})\n",
    "\n",
    "# Convert to DataFrame\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "\n",
    "# Display the first few tweets\n",
    "display(tweets_df[['created_at', 'user', 'text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b773ea",
   "metadata": {},
   "source": [
    "### Financial Forum: Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d431c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3292b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: None\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=st.secrets[\"CLIENT_ID\"],\n",
    "    client_secret=st.secrets[\"CLIENT_SECRET\"],\n",
    "    user_agent=st.secrets[\"USER_AGENT\"],\n",
    ")\n",
    "\n",
    "# Verify Reddit instance is connected\n",
    "print(f\"Connected as: {reddit.user.me()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caab2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily FI discussion thread - Friday, October 11, 2024\n",
      "The Official 2023 Survey Results Are Here\n",
      "$1M Checkmark (a culmination of lessons from Reddit)\n",
      "Is there a calculator or spreadsheet to help determine how to drawdown investments?\n",
      "4% of portfolio value VS 4% + inflation... which is the better withdrawal method?\n"
     ]
    }
   ],
   "source": [
    "# Fetch posts from a subreddit as a test\n",
    "subreddit = reddit.subreddit(\"financialindependence\")\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print(submission.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57c23767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Rank 1 keywords: ['HSI', 'Hang Seng', 'Hong Kong Stocks', 'Hong Kong Index', 'Hang Seng Index']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when HSI is going up he will donate</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/HongKong/comments/1fu...</td>\n",
       "      <td>1.727835e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another miserable year sees HSI decline 13.8 p...</td>\n",
       "      <td>12</td>\n",
       "      <td>https://news.rthk.hk/rthk/en/component/k2/1734...</td>\n",
       "      <td>1.703845e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSI rally shows market support for security la...</td>\n",
       "      <td>23</td>\n",
       "      <td>https://news.rthk.hk/rthk/en/component/k2/1530...</td>\n",
       "      <td>1.591451e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSI drops sharply over corporate profit warnin...</td>\n",
       "      <td>16</td>\n",
       "      <td>https://news.rthk.hk/rthk/en/component/k2/1509...</td>\n",
       "      <td>1.582021e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait a minuteâ€¦.</td>\n",
       "      <td>1620</td>\n",
       "      <td>https://i.redd.it/i8mboflbb0w91.jpg</td>\n",
       "      <td>1.666728e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>HongKong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0               when HSI is going up he will donate       0   \n",
       "1  Another miserable year sees HSI decline 13.8 p...     12   \n",
       "2  HSI rally shows market support for security la...     23   \n",
       "3  HSI drops sharply over corporate profit warnin...     16   \n",
       "4                                    Wait a minuteâ€¦.   1620   \n",
       "\n",
       "                                                 url   created_utc  \\\n",
       "0  https://www.reddit.com/r/HongKong/comments/1fu...  1.727835e+09   \n",
       "1  https://news.rthk.hk/rthk/en/component/k2/1734...  1.703845e+09   \n",
       "2  https://news.rthk.hk/rthk/en/component/k2/1530...  1.591451e+09   \n",
       "3  https://news.rthk.hk/rthk/en/component/k2/1509...  1.582021e+09   \n",
       "4                https://i.redd.it/i8mboflbb0w91.jpg  1.666728e+09   \n",
       "\n",
       "   num_comments subreddit  \n",
       "0             3  HongKong  \n",
       "1             3  HongKong  \n",
       "2             5  HongKong  \n",
       "3             0  HongKong  \n",
       "4            18  HongKong  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of subreddits to search\n",
    "SUBREDDITS = [\"HongKong\", \"stocks\", \"investing\", \"finance\"]\n",
    "\n",
    "def fetch_reddit_data(limit=5):\n",
    "    \"\"\"Fetches posts from specified subreddits based on ranked keywords.\"\"\"\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        print(f\"Searching Rank {rank} keywords: {keywords}\")\n",
    "        posts = []\n",
    "\n",
    "        for subreddit_name in SUBREDDITS:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            for keyword in keywords:\n",
    "                for submission in subreddit.search(keyword, limit=limit):\n",
    "                    posts.append({\n",
    "                        \"title\": submission.title,\n",
    "                        \"score\": submission.score,\n",
    "                        \"url\": submission.url,\n",
    "                        \"created_utc\": submission.created_utc,\n",
    "                        \"num_comments\": submission.num_comments,\n",
    "                        \"subreddit\": subreddit_name\n",
    "                    })\n",
    "                \n",
    "                # Stop searching lower ranks if we find posts\n",
    "                if posts:\n",
    "                    return pd.DataFrame(posts)\n",
    "        \n",
    "    # If no posts found in any rank\n",
    "    print(\"No Reddit posts found for any of the ranked keywords.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "fetch_reddit_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2151bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = {'text': 'JPMorgan Chase ( JPM ) earnings Q3 2024', 'tag': 'Financial News', 'sentiment': 'Positive', 'details': {'mood_agent': 'The mood of this message is neutral, with a slight hint of optimism. The use of speculative language like \"earnings\" and \"Q3 2024\" suggests that the information being discussed is still in the future and uncertain. However, the mention of JPMorgan Chase, a well-known and successful company, may evoke a sense of positivity or anticipation about its financial performance in the upcoming third quarter of 2024. Overall, the mood is measured but potentially hopeful.', 'institutional_investor_agent': \"As an institutional investor, I would analyze JPMorgan Chase's earnings report for Q3 2024 with a focus on long-term stability and growth potential. \\n\\nFirstly, I would examine the key financial metrics such as revenue, net income, and earnings per share to assess the bank's performance during the quarter. A strong set of financial results would indicate that the bank is able to generate consistent profits and maintain a solid financial position, which is crucial for long-term stability.\\n\\nI would also look at the bank's performance in key business segments such as consumer banking, investment banking, and asset management to determine the diversification of revenue streams. A well-diversified business mix can help the bank navigate through different economic cycles and reduce risk.\\n\\nFurthermore, I would pay close attention to any updates on the bank's strategic initiatives and plans for growth. This could include expansion into new markets, investments in technology and digital transformation, or acquisitions to strengthen the bank's competitive position. Positive developments in these areas could signal potential growth opportunities in the long term.\\n\\nOverall, as an institutional investor, I would analyze JPMorgan Chase's Q3 2024 earnings report to assess the bank's financial performance, strategic direction, and growth potential. A strong set of financial results and positive outlook for future growth would be key factors that would influence my investment decisions in the bank.\"}}, {'text': 'CNBC Daily Open : Is higher - than - expected CPI a  janky  data point ? ', 'tag': 'Financial News', 'sentiment': 'Negative', 'details': {'mood_agent': 'The mood of the message is cautious or skeptical. The use of the word \"janky\" to describe the data point suggests that there may be concerns about its reliability or accuracy. The mention of the data point being higher-than-expected also hints at the possibility of unexpected or undesirable outcomes. Overall, the message conveys a sense of uncertainty or doubt about the situation.', 'institutional_investor_agent': 'As an institutional investor focused on long-term stability and growth potential, it is important to take into account various economic indicators, including the Consumer Price Index (CPI). The message implies that there may be uncertainty or inconsistency in the CPI data, referring to it as \"janky.\" \\n\\nA higher-than-expected CPI could potentially signal inflationary pressures in the economy, which could have implications for stability and growth potential. Inflation can erode purchasing power, lead to higher interest rates, and impact consumer spending and overall economic growth. \\n\\nIt is important for institutional investors to closely monitor CPI data and other economic indicators to assess the overall health of the economy and make informed investment decisions. Any doubts about the accuracy or reliability of economic data could introduce additional uncertainty into the market, potentially affecting investment strategies and portfolio performance in the long term.'}}, {'text': 'when HSI is going up he will donate ', 'tag': 'Financial Forum Data', 'sentiment': 'Positive', 'details': {'mood_agent': '**Mood Analysis**: The response is positive and optimistic, reflecting a confident belief in the positive future of the Hang Seng Index (HSI) and a commitment to charitable contributions based on this expected performance.\\n\\n**Institutional Investor Analysis**: The statement indicates a potential reliance on short-term market movements for investment decisions, which could lead to increased market volatility and impact long-term stability. The practice of donating short-term gains also suggests a lack of a structured, long-term investment strategy, possibly affecting sustainable market growth.\\n\\n**Conclusion**: While the sentiment is positive, the approach described may not align with disciplined, long-term financial planning and could ultimately affect market stability.', 'institutional_investor_agent': '**Mood Analysis**: The response exudes positivity and optimism, demonstrated by the speaker\\'s confidence in using \"when\" instead of \"if\" regarding the Hang Seng Indexâ€™s future rise, and plans to donate the gains. This highlights a hopeful outlook and a commitment to philanthropy connected to financial success.\\n\\n**Institutional Investor Perspective**: The inclination to base investment and donation decisions on short-term market successes suggests a potentially volatile strategy. This approach might contribute to increased market volatility and lacks the disciplined, long-term perspective necessary for stable financial markets. It\\'s advisable for investors to engage in more strategic, fundamentally-driven planning to support sustained market health and avoid the pitfalls of reactive investment behaviors.'}}, {'text': 'Another miserable year sees HSI decline 13.8 percent', 'tag': 'Financial Forum Data', 'sentiment': 'Negative', 'details': {'mood_agent': 'The mood of the message is pessimistic and gloomy. The use of words like \"miserable\" and \"decline\" convey a negative sentiment. Additionally, the use of the word \"another\" suggests a pattern of negativity over time, adding to the overall sense of despondency. The speculative language (\"sees,\" \"could affect\") implies a sense of uncertainty about the future, adding to the overall somber mood of the message.', 'institutional_investor_agent': 'As an institutional investor, seeing the Hang Seng Index (HSI) decline 13.8 percent in a year would be concerning for long-term stability and growth potential. A significant decline in the HSI indicates volatility and potential economic challenges in the market. This could erode investor confidence and lead to further market downturns.\\n\\nThe decline in the HSI may impact the overall stability of the market and could hinder potential growth opportunities. It may also indicate broader economic issues that could affect the performance of investments in the region. As an institutional investor, it is important to closely monitor the situation, assess the underlying causes of the decline, and make informed decisions to mitigate risks and capitalize on potential opportunities.'}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
