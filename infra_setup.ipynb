{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec66573-9c1b-4f17-bcbe-787ddf067576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key not found. Please check the .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba6723-bf59-4f20-9dc8-856319891aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Instantiate the client using environment variable for the API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Make a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1e615-b023-4724-9838-708afc0e4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f842945-caf6-49ee-a682-b90aa99244b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mood_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the mood of this message, focusing on any hypothetical or speculative language that could affect sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def institutional_investor_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze this message as if you are an institutional investor, focusing on long-term impacts on stability and growth potential: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def individual_investor_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze this message as if you are an individual investor, focusing on short-term price impact and immediate gains or losses: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rhetoric_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the rhetorical style of this message, such as sarcasm, exaggeration, or assertive statements, and how these elements affect sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def dependency_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Focus on the speakerâ€™s sentiment in this message, without considering external perspectives or opinions of third parties: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def aspect_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the sentiment toward the main entity (e.g., company or stock ticker) in this message, ignoring unrelated information: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def reference_agent(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Identify references to time, price points, or external factors in this message, and analyze how they impact the overall sentiment: {content}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e817c-9884-446c-9e84-8725b5b39665",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"$SBUX this stock has been consolidating and coiling for years. I think many underestimate it. spring how\"\n",
    "\n",
    "# Run each agent on the content\n",
    "responses = {\n",
    "    \"mood_agent\": mood_agent(content),\n",
    "    \"institutional_investor_agent\": institutional_investor_agent(content),\n",
    "    \"individual_investor_agent\": individual_investor_agent(content),\n",
    "    \"rhetoric_agent\": rhetoric_agent(content),\n",
    "    \"dependency_agent\": dependency_agent(content),\n",
    "    \"aspect_agent\": aspect_agent(content),\n",
    "    \"reference_agent\": reference_agent(content)\n",
    "}\n",
    "\n",
    "# Display the responses from each agent\n",
    "for agent, response in responses.items():\n",
    "    print(f\"{agent}: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f3dd9-0bfd-442d-ac8a-1fd74210842a",
   "metadata": {},
   "source": [
    "# Mock Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf73599-d05e-4915-b837-eee6bf44013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial message for sentiment analysis\n",
    "content = \"$SBUX this stock has been consolidating and coiling for years. I think many underestimate it. spring how\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6d20dc-de06-4699-897a-68c05b1cc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock agent responses to simulate natural language sentiment descriptions\n",
    "def mood_agent(content):\n",
    "    return \"The mood seems cautiously optimistic about future potential.\"\n",
    "\n",
    "def institutional_investor_agent(content):\n",
    "    return \"This message suggests a long-term positive outlook due to consolidation.\"\n",
    "\n",
    "def individual_investor_agent(content):\n",
    "    return \"Indicates potential for short-term gains but remains uncertain.\"\n",
    "\n",
    "def rhetoric_agent(content):\n",
    "    return \"The language implies underestimation by others, hinting at overlooked growth.\"\n",
    "\n",
    "def dependency_agent(content):\n",
    "    return \"The speaker seems to hold a positive perspective independently.\"\n",
    "\n",
    "def aspect_agent(content):\n",
    "    return \"Focuses specifically on $SBUX, suggesting stability and possible growth.\"\n",
    "\n",
    "def reference_agent(content):\n",
    "    return \"No explicit timeframes, but consolidation indicates a buildup for potential movement.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4b2e87-a11a-42f9-b8d8-82b2519b3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect initial responses from each agent in a dictionary\n",
    "responses = {\n",
    "    \"mood_agent\": mood_agent(content),\n",
    "    \"institutional_investor_agent\": institutional_investor_agent(content),\n",
    "    \"individual_investor_agent\": individual_investor_agent(content),\n",
    "    \"rhetoric_agent\": rhetoric_agent(content),\n",
    "    \"dependency_agent\": dependency_agent(content),\n",
    "    \"aspect_agent\": aspect_agent(content),\n",
    "    \"reference_agent\": reference_agent(content)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "030a5628-175e-4e89-b5f4-1b82c34a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summative_agent(responses, max_rounds=2):\n",
    "    round_count = 0\n",
    "    consensus_reached = False\n",
    "    high_priority_agents = [\"institutional_investor_agent\", \"individual_investor_agent\"]\n",
    "    \n",
    "    # Store initial responses\n",
    "    sentiment_summary = {agent: response for agent, response in responses.items()}\n",
    "\n",
    "    while not consensus_reached and round_count < max_rounds:\n",
    "        # Step 1: Ask the model to summarize the collective sentiment based on agent responses\n",
    "        combined_responses = \"\\n\".join([f\"{agent}: {response}\" for agent, response in sentiment_summary.items()])\n",
    "        \n",
    "        # Ask GPT-4 to interpret the overall sentiment, allowing for Positive, Negative, Neutral, or Mixed\n",
    "        overall_sentiment = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Based on the following responses from various agents, summarize the overall sentiment as Positive, Negative, Neutral, or Mixed if no clear conclusion can be derived:\\n\\n{combined_responses}\"\n",
    "            }]\n",
    "        ).choices[0].message.content.strip().lower()\n",
    "        \n",
    "        # Check if a clear consensus is indicated by the response\n",
    "        if \"positive\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Positive\"\n",
    "        elif \"negative\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Negative\"\n",
    "        elif \"neutral\" in overall_sentiment:\n",
    "            consensus_reached = True\n",
    "            final_sentiment = \"Neutral\"\n",
    "        elif \"mixed\" in overall_sentiment or not consensus_reached:\n",
    "            # Inconclusive: proceed to the next round\n",
    "            round_count += 1\n",
    "            for agent, response in responses.items():\n",
    "                refined_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": f\"Refine your sentiment analysis by reviewing these responses:\\n{combined_responses}\\nOriginal response: {response}\"}\n",
    "                    ]\n",
    "                )\n",
    "                sentiment_summary[agent] = refined_response.choices[0].message.content\n",
    "\n",
    "    # If no consensus after max rounds, defer to high-priority agents for final decision\n",
    "    if not consensus_reached:\n",
    "        high_priority_responses = \"\\n\".join([f\"{agent}: {sentiment_summary[agent]}\" for agent in high_priority_agents if agent in sentiment_summary])\n",
    "        final_sentiment = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Summarize the final sentiment based on high-priority agents alone, as Positive, Negative, Neutral, or Mixed if no clear conclusion can be drawn:\\n{high_priority_responses}\"\n",
    "            }]\n",
    "        ).choices[0].message.content.strip().capitalize()\n",
    "\n",
    "    # Return the final sentiment and a detailed summary of agent responses\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab36fcf-fb24-464e-ad82-24a830e00dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the summative agent to determine final sentiment\n",
    "final_output = summative_agent(responses)\n",
    "print(\"Final Aggregated Sentiment:\", final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371320e9",
   "metadata": {},
   "source": [
    "# Financial Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6724a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKED_KEYWORDS = [\n",
    "    [\"Hong Kong\", \"HSI\", \"Hang Seng\"],  # Rank 1\n",
    "    [\"China\", \"Asia\", \"Singapore\"],      # Rank 2\n",
    "    [\"US\", \"Global\", \"Something\"]        # Rank 3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25488d18",
   "metadata": {},
   "source": [
    "### Financial News: GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "params = {\n",
    "    'query': '(economy OR finance)',  # Corrected query with parentheses\n",
    "    'mode': 'ArtList',\n",
    "    'format': 'JSON'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# Sending request to GDELT\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "# Verify the response and process the data\n",
    "if response.status_code == 200:\n",
    "    if response.text.strip():  # Check if the response is not empty\n",
    "        try:\n",
    "            data = response.json()\n",
    "            articles = pd.DataFrame(data.get('articles', []))\n",
    "            if 'title' in articles.columns:\n",
    "                # Displaying only the titles\n",
    "                display(articles['title'].head())\n",
    "            else:\n",
    "                print(\"No title information found in the response.\")\n",
    "        except ValueError:\n",
    "            print(\"Error decoding JSON. Response text:\", response.text)\n",
    "    else:\n",
    "        print(\"Received an empty response.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code, \"\\nResponse Text: \", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_data_with_ranking(max_records=10):\n",
    "    credible_sources = ['reuters.com', 'yahoo.com', 'cnbc.com']\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        query = f\"({ ' OR '.join(keywords) })\"\n",
    "        print(f\"Generated GDELT query: {query}\")\n",
    "        articles = fetch_gdelt_data(query, max_records)\n",
    "        print(articles)\n",
    "        if articles:\n",
    "            filtered_articles = [\n",
    "                {\n",
    "                    \"title\": article.get(\"title\"),\n",
    "                    \"url\": article.get(\"url\"),\n",
    "                    \"language\": article.get(\"language\"),\n",
    "                    \"sourcecountry\": article.get(\"sourcecountry\"),\n",
    "                    \"domain\": article.get(\"domain\")\n",
    "                }\n",
    "                for article in articles\n",
    "                if any(source in article.get('url', '') for source in credible_sources) and article.get('language', '').lower() == 'english'\n",
    "            ]\n",
    "            if filtered_articles:\n",
    "                print(f\"Found articles with Rank {rank} keywords from credible sources.\")\n",
    "                return filtered_articles\n",
    "\n",
    "    print(\"No articles found for any of the ranked keywords from credible sources.\")\n",
    "    return []\n",
    "\n",
    "def fetch_gdelt_data(query, max_records=10):\n",
    "    url = f\"https://api.gdeltproject.org/api/v2/doc/doc?query={query}&mode=artlist&maxrecords={max_records}&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        if response.headers.get('Content-Type') == 'application/json':\n",
    "            try:\n",
    "                data = response.json()\n",
    "                return data.get('articles', [])\n",
    "            except JSONDecodeError as e:\n",
    "                print(\"Error decoding JSON response from GDELT:\", e)\n",
    "                return []\n",
    "        else:\n",
    "            print(\"Unexpected content type from GDELT response:\", response.headers.get('Content-Type'))\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching GDELT data:\", e)\n",
    "        return []\n",
    "    \n",
    "\n",
    "fetch_gdelt_data_with_ranking(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90096d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the GDELT API endpoint\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Define the query parameters for the GDELT GEO 2.0 API\n",
    "params = {\n",
    "    \"query\": '(Hong Kong OR HSI OR \"Hang Seng\") AND (China OR Asia OR Singapore) AND (US OR Global OR Something) ' \\\n",
    "             'AND (domain:reuters.com OR domain:yahoo.com OR domain:cnbc.com) AND sourcelang:english',\n",
    "    \"mode\": \"ArtList\",          # To get a list of articles\n",
    "    \"format\": \"json\",           # To retrieve data in JSON format\n",
    "    \"maxrecords\": \"10\",         # Limit to 10 records for testing\n",
    "    \"timespan\": \"7d\",           # Only articles from the past week\n",
    "    \"sort\": \"DateDesc\"          # Sort by most recent articles first\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Print the response to inspect or troubleshoot\n",
    "print(response.text)\n",
    "\n",
    "# Process the JSON response if successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data.get('articles', [])\n",
    "    for article in articles:\n",
    "        print(f\"Title: {article.get('title')}, URL: {article.get('url')}\")\n",
    "else:\n",
    "    print(\"Error:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_data_with_ranking(max_records=10):\n",
    "    \"\"\"Fetches articles from GDELT based on ranked keywords.\"\"\"\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        # Adjusted without quotes around OR terms\n",
    "        keyword_query = \"(\" + \" OR \".join(keywords) + \")\"\n",
    "        query = f'{keyword_query} AND domain:reuters.com AND sourcelang:english'\n",
    "        print(f\"Trying Rank {rank} keywords: {query}\")\n",
    "        articles = fetch_gdelt_data(query, max_records)\n",
    "        if articles:\n",
    "            print(f\"Found articles with Rank {rank} keywords.\")\n",
    "            return pd.DataFrame(articles)\n",
    "    print(\"No articles found.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def fetch_gdelt_data(query, max_records=10):\n",
    "    \"\"\"Helper function to retrieve GDELT data for a specific query.\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"format\": \"json\",\n",
    "        \"maxrecords\": max_records,\n",
    "        \"sort\": \"DateDesc\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        \n",
    "        # Print available fields in each article for inspection\n",
    "        for article in articles:\n",
    "            print(article.keys())  # Print keys for each article\n",
    "            article[\"content\"] = article.get(\"excerpt\", \"Content not available\")  # Use 'excerpt' if 'content' is missing\n",
    "            \n",
    "        return articles\n",
    "    else:\n",
    "        print(\"Error fetching GDELT data:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    \n",
    "print(\"\\nTesting GDELT data retrieval...\")\n",
    "gdelt_data = fetch_gdelt_data_with_ranking()\n",
    "display(gdelt_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f65662",
   "metadata": {},
   "source": [
    "### SNS Financial Data: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da381b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Set your keys and tokens (replace placeholders with your actual credentials)\n",
    "consumer_key = \"YOUR_CONSUMER_KEY\"\n",
    "consumer_secret = \"YOUR_CONSUMER_SECRET\"\n",
    "access_token = \"YOUR_ACCESS_TOKEN\"\n",
    "access_token_secret = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Fetch tweets with the specified keyword\n",
    "tweets = []\n",
    "for tweet in tweepy.Cursor(api.search_tweets, q=\"economy\", lang=\"en\", tweet_mode=\"extended\").items(10):\n",
    "    tweets.append({'created_at': tweet.created_at, 'user': tweet.user.screen_name, 'text': tweet.full_text})\n",
    "\n",
    "# Convert to DataFrame\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "\n",
    "# Display the first few tweets\n",
    "display(tweets_df[['created_at', 'user', 'text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b773ea",
   "metadata": {},
   "source": [
    "### Financial Forum: Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3292b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as: Nnfts\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    password=os.getenv(\"PASSWORD\"),  # Replace with your Reddit account password\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    username=os.getenv(\"USERNAME\").strip()\n",
    ")\n",
    "\n",
    "# Verify Reddit instance is connected\n",
    "print(f\"Connected as: {reddit.user.me()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch posts from a subreddit as a test\n",
    "subreddit = reddit.subreddit(\"financialindependence\")\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print(submission.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57c23767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Rank 1 keywords: ['Hong Kong', 'HSI', 'Hang Seng']\n"
     ]
    },
    {
     "ename": "Redirect",
     "evalue": "Redirect to /subreddits/search",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRedirect\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Reddit posts found for any of the ranked keywords.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mfetch_reddit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 16\u001b[0m, in \u001b[0;36mfetch_reddit_data\u001b[1;34m(limit)\u001b[0m\n\u001b[0;32m     14\u001b[0m subreddit \u001b[38;5;241m=\u001b[39m reddit\u001b[38;5;241m.\u001b[39msubreddit(subreddit_name)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubreddit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubreddit_name\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Stop searching lower ranks if we find posts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\models\\listing\\generator.py:63\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\models\\listing\\generator.py:89\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_sublist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;129m@_deprecate_args\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    704\u001b[0m     params: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m ):\n\u001b[0;32m    706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objectify_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objector\u001b[38;5;241m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hangyul Son\\TestingEnvironment\\LLMBasedTrading\\venv\\Lib\\site-packages\\prawcore\\sessions.py:267\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_retry(\n\u001b[0;32m    255\u001b[0m         data,\n\u001b[0;32m    256\u001b[0m         files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m         url,\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRedirect\u001b[0m: Redirect to /subreddits/search"
     ]
    }
   ],
   "source": [
    "# List of subreddits to search\n",
    "SUBREDDITS = [\"HongKong\", \"stocks\", \"investing\", \"finance\"]\n",
    "\n",
    "def fetch_reddit_data(limit=5):\n",
    "    \"\"\"Fetches posts from specified subreddits based on ranked keywords.\"\"\"\n",
    "    for rank, keywords in enumerate(RANKED_KEYWORDS, start=1):\n",
    "        print(f\"Searching Rank {rank} keywords: {keywords}\")\n",
    "        posts = []\n",
    "\n",
    "        for subreddit_name in SUBREDDITS:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            for keyword in keywords:\n",
    "                for submission in subreddit.search(keyword, limit=limit):\n",
    "                    posts.append({\n",
    "                        \"title\": submission.title,\n",
    "                        \"score\": submission.score,\n",
    "                        \"url\": submission.url,\n",
    "                        \"created_utc\": submission.created_utc,\n",
    "                        \"num_comments\": submission.num_comments,\n",
    "                        \"subreddit\": subreddit_name\n",
    "                    })\n",
    "                \n",
    "                # Stop searching lower ranks if we find posts\n",
    "                if posts:\n",
    "                    return pd.DataFrame(posts)\n",
    "        \n",
    "    # If no posts found in any rank\n",
    "    print(\"No Reddit posts found for any of the ranked keywords.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "fetch_reddit_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
